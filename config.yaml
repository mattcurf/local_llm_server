healthCheckTimeout: 600
logLevel: info
startPort: 10001

macros:
  "llama-server-base": |
    /usr/local/bin/llama-server \
    --port ${PORT} \
    --no-warmup \
    --threads -1 \

  "llama-qwen-instruct": |
    /usr/local/bin/llama-server \
    --port ${PORT} \
    --no-warmup \
    --threads -1 \
    --ctx-size 262144 \
    --n-gpu-layers 99 \
    -ctk q4_0 \
    -ctv q4_0 \
    --temp 0.7 \
    --min-p 0.0 \
    --top-p 0.8 \
    --top-k 20

  "llama-qwen-thinking": |
    /usr/local/bin/llama-server \
    --port ${PORT} \
    --no-warmup \
    --threads -1 \
    --ctx-size 262144 \
    --n-gpu-layers 99 \
    -ctk q4_0 \
    -ctv q4_0 \
    --temp 0.6 \
    --min-p 0.0 \
    --top-p 0.95 \
    --top-k 20
 
  "glm": |
    /usr/local/bin/llama-server \
    --port ${PORT} \
    --no-warmup \
    --threads -1 \
    --ctx-size 131072 \
    --n-gpu-layers 99 \
    -ctk q4_0 \
    -ctv q4_0 \
    --jinja 

  "gpt-oss": |
    /usr/local/bin/llama-server \
    --port ${PORT} \
    --no-warmup \
    --threads -1 \
    --n-gpu-layers 99 \
    --jinja \
    --temp 1.0 \
    --min-p 0.0 \
    --top-p 1.0 \
    --top-k 0

models:
  "qwen3-30b-instruct-q4":
    cmd: |
      ${llama-qwen-instruct}
      --model /models/Qwen3-30B-A3B-Instruct-2507-GGUF/Qwen3-30B-A3B-Instruct-2507-UD-Q4_K_XL.gguf
    ttl: 300 

  "qwen3-30b-thinking-q4":
    cmd: |
      ${llama-qwen-thinking} 
      --model /models/Qwen3-30B-A3B-Thinking-2507-GGUF/Qwen3-30B-A3B-Thinking-2507-UD-Q4_K_XL.gguf
    ttl: 300

  "qwen3-235b-instruct-q4":
    cmd: |
      ${llama-qwen-instruct}
      -ot ".ffn_.*_exps.=CPU" \
      --model /models/Qwen3-235B-A22B-Instruct-2507-GGUF/UD-Q4_K_XL/Qwen3-235B-A22B-Instruct-2507-UD-Q4_K_XL-00001-of-00003.gguf 
    ttl: 300

  "qwen3-235b-thinking-q4":
    cmd: |
      ${llama-qwen-thinking} 
      -ot ".ffn_.*_exps.=CPU" \
      --model /models/Qwen3-235B-A22B-Thinking-2507-GGUF/UD-Q4_K_XL/Qwen3-235B-A22B-Thinking-2507-UD-Q4_K_XL-00001-of-00003.gguf 
    ttl: 300

  "qwen3-30b-coder-q4":
    cmd: |
      ${llama-qwen-instruct}
      --repeat-penalty 1.05 \
      --model /models/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL.gguf 
    ttl: 86400

  "qwen3-480Bb-coder-q2":
    cmd: |
      ${llama-qwen-instruct}
      --repeat-penalty 1.05 \
      -ot ".ffn_.*_exps.=CPU" \
      --model /models/Qwen3-Coder-480B-A35B-Instruct-GGUF/UD-Q2_K_XL/Qwen3-Coder-480B-A35B-Instruct-UD-Q2_K_XL-00001-of-00004.gguf 
    ttl: 300

  "glm-4-32b-q4":
    cmd: |
      ${glm}
      --model /models/GLM-4-32B-0414-GGUF/GLM-4-32B-0414-UD-Q4_K_XL.gguf  
    ttl: 300

  "glm-4.5-air-106b-q4":
    cmd: |
      ${glm}
      -ot ".ffn_.*_exps.=CPU" \
      --model /models/GLM-4.5-Air-GGUF/UD-Q4_K_XL/GLM-4.5-Air-UD-Q4_K_XL-00001-of-00002.gguf
    ttl: 300

  "glm-4.5-355b-q4":
    cmd: |
      ${glm}
      -ot ".ffn_.*_exps.=CPU" \
      --model /models/GLM-4.5-GGUF/UD-Q4_K_XL/GLM-4.5-UD-Q4_K_XL-00001-of-00005.gguf
    ttl: 300

  "gpt-oss-20b":
    cmd: |
      ${gpt-oss}
      --ctx-size 16384 \
      --model /models/gpt-oss-20b-GGUF/gpt-oss-20b-UD-Q8_K_XL.gguf 
    ttl: 300

  "gpt-oss-120b":
    cmd: |
      ${gpt-oss}
      --ctx-size 262114 \
      -ot ".ffn_.*_exps.=CPU" \
      --model /models/gpt-oss-120b-GGUF/UD-Q8_K_XL/gpt-oss-120b-UD-Q8_K_XL-00001-of-00002.gguf
    ttl: 300


  "deepseek-v31-671b-tq1":
    cmd: |
      ${llama-server-base}
      --ctx-size 16384 \
      -ot ".ffn_.*_exps.=CPU" \
      --model /models/DeepSeek-V3.1-GGUF/DeepSeek-V3.1-UD-TQ1_0.gguf
      --cache-type-k q4_0 \
      --jinja \
      --n-gpu-layers 99 \
      --temp 0.6 \
      --top_p 0.95 \
      --min_p 0.01 
    ttl: 300

  "deepseek-v31-671b-q2":
    cmd: |
      ${llama-server-base}
      --ctx-size 16384 \
      -ot ".ffn_.*_exps.=CPU" \
      --model /models/DeepSeek-V3.1-GGUF/UD-Q2_K_XL/DeepSeek-V3.1-UD-Q2_K_XL-00001-of-00006.gguf \
      --cache-type-k q4_0 \
      --jinja \
      --n-gpu-layers 99 \
      --temp 0.6 \
      --top_p 0.95 \
      --min_p 0.01 
    ttl: 300

groups:
  "main-swap-group":
    swap: true
    exclusive: true
    members:
      - "qwen3-30b-instruct-q4"
      - "qwen3-30b-thinking-q4"
      - "qwen3-235b-instruct-q4"
      - "qwen3-235b-thinking-q4"
      - "qwen3-30b-coder-q4"
      - "qwen3-480b-coder-q2"
      - "glm-4-32b-q4"
      - "glm-4.5-air-106b-q4"
      - "glm-4.5-355b-q4"
      - "gpt-oss-20b"
      - "gpt-oss-120b"
      - "deepseek-v31-671b-tq1"
      - "deepseek-v31-671b-q2"

hooks:
  on_startup:
    preload:
      - "qwen3-30b-coder-q4"
